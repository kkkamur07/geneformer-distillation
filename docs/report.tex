\documentclass[12pt, a4paper]{article}

% Essential Packages
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{graphicx}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[backend=biber, style=ieee]{biblatex}

% Add your bibliography file here
\addbibresource{references.bib}

% Styling
\onehalfspacing
\setlength{\parskip}{1em}
\setlength{\parindent}{0em}

% Title Page Data
\title{\textbf{Transfer learning enables predictions in network biology}}
\author{
    \textbf{Anshul Saini and Krrish Agarwalla} \\
    \textit{Seminar on (Large) Language Model Applications for Text and Biological Data} \\
    Supervisor: Roberto Olayo Alarcon \\
    \\
    Ludwig-Maximilians-Universität München (LMU)
}
\date{\today}

\begin{document}

% -------------------------------------------------------------------
% TITLE PAGE
% -------------------------------------------------------------------
\maketitle
\thispagestyle{empty}
\newpage

% -------------------------------------------------------------------
% ABSTRACT
% -------------------------------------------------------------------
\begin{abstract}
    The application of large language models (LLMs) to biological data has revolutionized the analysis of single-cell transcriptomics. This report presents a reproduction and extension of \textit{Geneformer}, a context-aware, attention-based deep learning model pretrained on 30 million single-cell transcriptomes \cite{theodoris2023transfer}. Geneformer leverages transfer learning to make predictions in network biology settings with limited data, such as gene dosage sensitivity and chromatin dynamics \cite{theodoris2023transfer}.
    
    While the original authors demonstrated state-of-the-art performance, the computational cost of training and deploying such models remains a barrier. In this project, we reproduce the core findings of Geneformer, validating its rank-value encoding, pretraining strategies, and diverse fine-tuning methodologies, including dosage sensitivity predictions, chromatin dynamics modeling, and \textit{in silico} perturbation analysis for therapeutic discovery \cite{theodoris2023transfer}. Furthermore, we apply \textbf{Knowledge Distillation (KD)} to compress the model for resource-constrained environments. We successfully distilled the 10-million parameter Teacher model into multiple lightweight Student models with 4.3M, 3M, and 2M parameters. Our results demonstrate that the 4.3M distilled model retains 98\% of the teacher's performance (84.53\% vs 86.14\% accuracy) on the downstream task of classifying Cardiomyopathy phenotypes, while requiring significantly fewer computational resources. This work paves the way for the democratization of genomic foundation models on consumer-grade hardware.
\end{abstract}
\newpage

% -------------------------------------------------------------------
% 1. GENEFORMER: EXPLANATION AND CONTEXT
% -------------------------------------------------------------------
\section{Geneformer: Explanation and Context}

\subsection{The Biological Context: The Need for Transfer Learning}
Mapping and identifying key gene regulators within regulatory networks is fundamental to understanding disease mechanisms. However, mapping these architectures often requires massive amounts of transcriptomic data to learn the network dynamics between genes, which impedes discoveries in rare diseases or clinically inaccessible tissues.

Standard supervised learning trains a new model from scratch for each specific task. In contrast, \textbf{Transfer Learning} allows a model to learn fundamental knowledge from a large, general dataset (pretraining) and apply it to a specific task with limited data (fine-tuning). Geneformer applies this concept to transcriptomics, treating a single cell as a "sentence" and genes as "words" to model context-specific network dynamics.

\subsection{Data Curation: Genecorpus-30M}
The model is built upon \textit{Genecorpus-30M}, a large-scale pretraining corpus comprising 29.9 million human single-cell transcriptomes aggregated from 561 public datasets (including CellxGene, PanglaoDB, and others) \cite{theodoris2023transfer}. The curation process involves a rigorous, standardized pipeline to ensure data quality and comparability across diverse sequencing platforms:

\begin{itemize}
    \item \textbf{Filtering Criteria:} To remove low-quality data, cells were excluded based on three specific metrics:
    \begin{itemize}
        \item \textit{Mitochondrial Content:} Cells with mitochondrial read percentages greater than 3 standard deviations above the mean within a dataset were removed, as high mitochondrial content typically indicates cell death (apoptosis) or membrane rupture \cite{theodoris2023transfer}.
        \item \textit{Read Depth:} Cells with total read counts outside of 3 standard deviations from the dataset mean were excluded to filter out empty droplets or potential doublets \cite{theodoris2023transfer}.
        \item \textit{Gene Count:} Cells with fewer than seven detected protein-coding or miRNA genes were removed, as the model's 15\% masking objective would be ineffective on such sparse data \cite{theodoris2023transfer}.
    \end{itemize}
    
    \item \textbf{Exclusion of High Mutational Burden:} Cells with high mutational burdens, such as malignant cancer cells or immortalized cell lines, were deliberately excluded. This ensures the model learns "normal" network topology without the confounding effects of substantial network rewiring found in cancer \cite{theodoris2023transfer}.
    
    \item \textbf{Normalization Strategy:} To handle technical variation across droplet-based sequencing platforms, raw transcript counts were normalized by the total read count per cell. This accounts for varying sequencing depths between experiments while preserving the relative abundance of genes \cite{theodoris2023transfer}.
\end{itemize}

Ultimately, 27.4 million cells passed these quality filters to form the final training corpus. The data structure includes gene IDs, read counts, and extensive metadata (organ, sequencing platform, etc.), as visualized below.

% FIGURE 1: Data Example (Changed to [htbp] to allow floating)
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{dataset_example.png}
    \caption{Example of the transcriptomic dataset structure. Each row represents a gene's expression within a specific cell, annotated with organ and quality control metrics \cite{agarwalla2023presentation}.}
    \label{fig:dataset_example}
\end{figure}

% FIGURE 2: Pipeline
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{data_engineering.png}
    \caption{The data curation pipeline used to generate Genecorpus-30M \cite{agarwalla2023presentation}. Raw data is aggregated, filtered for quality control, and passed to Rank Value Encoding.}
    \label{fig:pipeline}
\end{figure}

\subsection{Rank Value Encoding}
A critical innovation of Geneformer is the \textbf{Rank Value Encoding} \cite{theodoris2023transfer}. Traditional transcriptomic analyses often rely on raw expression counts, which can be heavily biased by ubiquitously expressed "housekeeping" genes. These genes consume a large proportion of sequencing reads but often provide little information about the specific cellular state or disease phenotype. To mitigate this bias and focus the model's attention on biologically informative regulatory genes (transcription factors), Geneformer employs a rank-based input representation.

The encoding process transforms raw data into a sequence of tokens through the following mathematical steps \cite{agarwalla2023presentation}:

\begin{enumerate}
    \item \textbf{Normalization:} First, the raw read count of a gene is normalized by the sequencing depth of that cell (total reads) and scaled by a gene-specific median factor. For a gene $g$ in cell $c$, the normalized value $V_{gc}$ is calculated as:
    \begin{equation}
        V_{gc} = \frac{R_{gc}}{D_c} \times \frac{10,000}{M_g}
    \end{equation}
    Where:
    \begin{itemize}
        \item $R_{gc}$ is the raw read count of gene $g$ in cell $c$.
        \item $D_c$ is the total read depth of cell $c$.
        \item $M_g$ is the non-zero median expression of gene $g$ across the entire Genecorpus-30M.
    \end{itemize}
    This step effectively deprioritizes housekeeping genes, which typically have high median expression ($M_g$), thereby lowering their normalized value $V_{gc}$.

    \item \textbf{Ranking:} The genes within each cell are then sorted in descending order based on their normalized values $V_{gc}$.
    \begin{equation}
        \text{Rank}(c) = \text{argsort}_{\text{desc}}(V_{1c}, V_{2c}, ..., V_{nc})
    \end{equation}
    
    \item \textbf{Tokenization:} The resulting ranked list of genes is converted into a sequence of tokens. Highly expressed, cell-state-defining genes (like transcription factors) appear earlier in the sequence, while housekeeping genes are pushed towards the end \cite{theodoris2023transfer}.
\end{enumerate}

This rank-based representation is robust to variations in sequencing depth and ensures that the model learns the hierarchical structure of gene regulation rather than absolute expression levels.

\subsection{Model Architecture and Pretraining}
Geneformer is built upon a standard Transformer encoder architecture, specifically adapted for the constraints of single-cell data. The model consists of a stack of six transformer encoder units, a depth chosen based on the maximum data scale available for effective pretraining.

The specific architectural hyperparameters are as follows:
\begin{itemize}
    \item \textbf{Input Size:} 2,048 tokens. This length was selected to fully represent 93\% of the rank value encodings in the Genecorpus-30M dataset.
    \item \textbf{Embedding Dimension ($d_{model}$):} 256. Each gene token is embedded into a 256-dimensional vector space.
    \item \textbf{Attention Heads:} 4 heads per layer.
    \item \textbf{Feed-Forward Dimension ($d_{ff}$):} 512.
    \item \textbf{Regularization:} A dropout probability of 0.02 is applied to fully connected layers and attention probabilities to prevent overfitting.
\end{itemize}

The model utilizes full dense self-attention across the entire input sequence. Training was optimized using the AdamW optimizer with a linear learning rate scheduler (max learning rate $1 \times 10^{-3}$) and a warmup period of 10,000 steps in the paper.

\subsubsection{Attention Mechanism: Learning Network Dynamics}
The core of Geneformer is the multi-head self-attention mechanism, which enables the model to be ``context-aware.'' In natural language processing, attention allows a model to understand that the word ``bank'' has a different meaning in the context of ``river'' versus ``money.'' Analogously, Geneformer uses attention to learn how the function of a specific gene changes based on the co-expression of other regulators in the cell.

The attention weights ($A$) for a given gene reflect which other genes it ``attends'' to (inputs) and which genes attend to it (regulatory influence). Analysis of the pretrained weights revealed that the model learns biological hierarchies in a completely self-supervised manner:
\begin{itemize}
    \item \textbf{Transcription Factor Specialization:} Approximately 20\% of the attention heads significantly prioritize transcription factors (TFs) over other genes, recognizing their role as global regulators.
    \item \textbf{Layer-wise Hierarchy:} The model exhibits a functional hierarchy across its six layers. Early layers tend to survey the broad input space (diverse attention), while deeper layers become ``centrality-driven,'' focusing heavily on the highest-ranked genes that uniquely define the cell state.
\end{itemize}
This demonstrates that the attention mechanism successfully encodes the gene regulatory network topology without any prior biological labeling.

\subsubsection{Masked Language Modeling (MLM)}
The model is pretrained using a \textbf{Masked Learning Objective}, similar to BERT in NLP \cite{theodoris2023transfer}.
\begin{itemize}
    \item \textbf{Process:} 15\% of the genes in a sequence are randomly masked \cite{theodoris2023transfer}.
    \item \textbf{Objective:} The model must predict the identity of the masked genes based solely on the context of the remaining unmasked genes.
\end{itemize}
This forces the model to learn the fundamental "grammar" of gene networks in a self-supervised manner \cite{theodoris2023transfer}.

% FIGURE 3: MASKING (LOCKED WITH [H] TO PREVENT FLOATING)
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{comparsion_bert_geneformer.png}
    \caption{Geneformer utilizes a masking strategy where 15\% of gene tokens are masked, forcing the model to learn network representations \cite{agarwalla2023presentation}.}
    \label{fig:masking}
\end{figure}

% -------------------------------------------------------------------
% 2. YOUR IDEAS: KNOWLEDGE DISTILLATION
% -------------------------------------------------------------------
\section{Our Contribution: Knowledge Distillation}

\subsection{Problem Statement}
While Geneformer is the state of the art model, it is computationally expensive. The original model contains 10 million parameters and required training on 12 V100 GPUs for days \cite{agarwalla2023presentation}. This creates a barrier to entry for researchers with limited resources. Additionally, the dataset is massive (30M rows), causing standard training loops to crash RAM on consumer hardware \cite{agarwalla2023presentation}.

\subsection{Proposed Solution: Teacher-Student Distillation}
To address this, we implemented \textbf{Knowledge Distillation (KD)} \cite{agarwalla2023presentation}. This technique compresses the knowledge of a large "Teacher" model into a smaller, lighter "Student" model.

Knowledge Distillation can be understood through an intuitive analogy: consider an experienced teacher guiding a student preparing for an exam. The teacher doesn't simply tell the student "the answer is C" instead, they explain their reasoning: "C is most likely correct, but B is also plausible if you interpret the question differently, while A and D are clearly wrong for these reasons." This nuanced guidance helps the student understand not just what is correct, but why, and how to think about similar problems in the future.

Similarly, in KD, a large pre-trained Teacher model transfers its learned knowledge to a compact Student model by sharing not just the final predictions, but the full probability distributions over all possible outputs. For instance, when classifying an image of a husky, the Teacher might output: husky (70\%), wolf (20\%), German shepherd (8\%), other (2\%). This probability distribution reveals that the model has learned meaningful visual similarities between related classes huskies do look more like wolves than like cats information that would be completely lost if we only provided the hard label "husky" with 100\% confidence.

The effectiveness of knowledge distillation stems from this "dark knowledge" the implicit understanding of relationships between classes that emerges from extensive training. When the Student learns to mimic the Teacher's probability distributions, it inherits this \textit{refined understanding without needing to see as many training examples or make the same mistakes.}

Our goal was to train a Student model with approximately 40-50\% of the parameters of the original, using only $1/25^{th}$ of the compute resources, while maintaining comparable accuracy. 

% FIGURE 4: DISTILLATION
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{distillation.png}
    \caption{Schematic overview of the knowledge distillation process. The Student model learns from both the ground truth labels and the Teacher's probability distributions \cite{agarwalla2023presentation}.}
    \label{fig:distillation}
\end{figure}

\subsection{Distillation Methodology}
To investigate the limits of model compression, we established a distillation framework involving a frozen Teacher and multiple variable-sized Student models \cite{agarwalla2023presentation}.

\begin{enumerate}
    \item \textbf{Teacher Model:} The original pretrained Geneformer (10M parameters). This model remains frozen during training to provide stable soft-label targets.
    \item \textbf{Student Models:} We trained three distinct student variants to compare performance across different compression ratios:
    \begin{itemize}
        \item \textbf{4.3M Parameters} ($\sim$43\% of Teacher size)
        \item \textbf{3M Parameters} ($\sim$30\% of Teacher size)
        \item \textbf{2M Parameters} ($\sim$20\% of Teacher size)
    \end{itemize}
\end{enumerate}

\textbf{Parameter Counting Methodology:}
The reported parameter counts include all trainable weights in the model: token embeddings, positional encodings, transformer encoder blocks (Self-Attention and Feed-Forward networks), and the classification head. 

To achieve these reductions, we decreased the network depth (number of transformer layers) and the embedding dimension from $d_{\text{model}} = 256$ to $d_{\text{model}} \in \{128, 96, 64\}$, as the total parameter count is dictated by the formula for BeRT style models:

\begin{equation}
    N_{\text{params}} = 2 \cdot N_{\text{vocab}} \cdot d_{\text{model}} + N_{\text{layers}} \cdot \left(4 d_{\text{model}}^2 + 2 d_{\text{model}} \cdot d_{\text{ff}}\right)
\end{equation}

where $N_{\text{vocab}}$ is the vocabulary size, $N_{\text{layers}}$ is the number of transformer layers, $d_{\text{model}}$ is the embedding dimension, and $d_{\text{ff}}$ is the feed-forward hidden dimension (typically $d_{\text{ff}} = 4 \cdot d_{\text{model}}$). 

\subsubsection{Loss Function Design}
The Student is trained using a composite loss function that combines two distinct objectives \cite{agarwalla2023presentation}:
\begin{equation}
    L_{total} = \alpha L_{CE} + (1-\alpha) L_{KL}
\end{equation}

Where:
\begin{itemize}
    \item $L_{CE}$ (Cross Entropy Loss): This measures the error between the Student's hard predictions ($p_{student}$) and the ground truth labels ($y$). It ensures the student learns the "right answer" by penalizing incorrect classifications.
    \begin{equation}
        L_{CE} = - \sum_{i=1}^{C} y_{i} \log(p_{student, i})
    \end{equation}
    
    \item $L_{KL}$ (Kullback-Leibler Divergence): This measures the distributional distance between the Student's soft predictions ($P_S$) and the Teacher's soft labels ($P_T$). Unlike Cross Entropy, which focuses on the single correct class, KL Divergence forces the Student to mimic the Teacher's uncertainty and secondary probabilities. This transfers the "dark knowledge" the structural relationships between classes (e.g., that Class A is more similar to Class B than to Class C) \cite{agarwalla2023presentation}.
    \begin{equation}
        L_{KL}(P_T || P_S) = \sum_{i=1}^{C} p_{teacher, i} \log \left( \frac{p_{teacher, i}}{p_{student, i}} \right)
    \end{equation}
\end{itemize}
By learning from the Teacher's soft labels, the Student learns the "dark knowledge" the subtle relationships between classes that the Teacher has learned.

For the distillation process, we set the loss weighting coefficient $\alpha = 0.65$ to balance the contributions of soft predictions from the Teacher model and hard ground-truth labels. It should be noted that no systematic hyperparameter optimization was conducted in this study. We believe that comprehensive hyperparameter tuning, including optimization of $\alpha$, temperature $T$ \& learning rate  would yield substantial improvements in Student model performance. 

\subsection{Engineering Optimizations}
To enable training on a single GPU, we implemented critical engineering optimizations to handle the computational constraints:

\subsubsection{Dynamic Length-Grouped Sampling}
Standard training pipelines typically sample batches randomly. However, as illustrated in Figure \ref{fig:seq_dist}, the Genecorpus-30M dataset exhibits a highly skewed distribution of sequence lengths. While the maximum input size is 2,048 tokens, the median sequence length is only 234 tokens \cite{agarwalla2023presentation}.

In a standard random batch, if a single long sequence (e.g., 2,048) is paired with many short sequences (e.g., 200), all short sequences must be padded with zeros to match the longest one. This results in the GPU spending the vast majority of its compute power processing empty "pad" tokens.

To solve this, **Dynamic Length-Grouped Sampling** was implemented:
\begin{enumerate}
    \item \textbf{Sorting:} We gather a large "megabatch" of samples and sort them by sequence length.
    \item \textbf{Grouping:} We construct minibatches using sequences of similar lengths.
    \item \textbf{Dynamic Padding:} Each minibatch is padded only to the longest sequence \textit{within that specific batch}, rather than the global maximum.
\end{enumerate}

This strategy reduced padding overhead by 62\% and resulted in a **29.4x speedup** compared to standard uniform sampling \cite{agarwalla2023presentation}.

% FIGURE: SEQUENCE DISTRIBUTION
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{sequence_distribution.png}
    \caption{Distribution of Sequence Lengths in Genecorpus-30M. The stark contrast between the median length (234) and the maximum length (2048) highlights the inefficiency of random batching and the necessity of length-grouped sampling \cite{agarwalla2023presentation}.}
    \label{fig:seq_dist}
\end{figure}

% -------------------------------------------------------------------
% 3. RESULTS
% -------------------------------------------------------------------
\section{Results and Discussion}

\subsection{Initial Evaluation: Pretraining Metrics}
Before fine-tuning on specific disease classification tasks, it was critical to verify that the Student model had successfully acquired the fundamental "grammar" of gene regulation from the Teacher. We evaluated this using two key metrics on the held-out validation set:
\begin{itemize}
    \item \textbf{Masked Language Modeling (MLM) Accuracy:} Measures how often the model correctly predicts a masked gene based on the context of the surrounding genes. High accuracy indicates a strong grasp of network topology.
    \item \textbf{Perplexity:} Measures the model's uncertainty in predicting the next token (lower is better).
\end{itemize}

Evaluating these metrics ensures that the Knowledge Distillation process successfully transferred the generalizable biological knowledge, rather than just overfitting to a specific downstream task. As shown in Table \ref{tab:pretraining}, the distilled Student model (4.3M parameters) achieves performance comparable to the Teacher (10M parameters). The marginal gap in accuracy (0.04) and perplexity (6.08) confirms that the Student retains the core structural understanding required for biological inference \cite{agarwalla2023presentation}.

\begin{table}[H]
    \centering
    \begin{tabular}{lccc}
        \toprule
        \textbf{Metric} & \textbf{Teacher} & \textbf{Student} & \textbf{Gap} \\
        \midrule
        MLM Accuracy & 0.3072 & 0.2606 & 0.0466 \\
        Perplexity & 15.28 & 21.36 & 6.08 \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of Pretraining Metrics. The Student retains significant linguistic capability despite a $\sim$50\% reduction in parameters \cite{agarwalla2023presentation}.}
    \label{tab:pretraining}
\end{table}

\subsection{Experimental Setup: Cardiomyopathy Classification}
Having validated the Student's general representational power, we validated our approach on a downstream task of classifying cardiomyocytes into three phenotypes \cite{agarwalla2023presentation}:
\begin{enumerate}
    \item \textbf{Non-Failing (NF):} Healthy control tissue.
    \item \textbf{Hypertrophic Cardiomyopathy (HCM):} Thickened heart muscle.
    \item \textbf{Dilated Cardiomyopathy (DCM):} Enlarged heart chamber.
\end{enumerate}
The dataset consisted of $\sim$132,000 cells \cite{agarwalla2023presentation}. To prevent data leakage and memorization, we split the data by Patient ID rather than by cell (Train: Patients A \& B; Test: Patient C) \cite{agarwalla2023presentation}.

\subsection{Teacher Model Performance}
The original Teacher model achieved high accuracy on this task. As shown in the confusion matrix and heatmap below (Figure \ref{fig:teacher_results}), it correctly classifies the majority of HCM (88\%) and DCM (84\%) cases with high confidence.

% FIGURE 5: TEACHER RESULTS
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{teacher_results.png}
    \caption{Teacher Model Results. The confusion matrix (left) confirms high precision, particularly for HCM (96\%). The heatmap (right) shows confident separation between cell states \cite{agarwalla2023presentation}.}
    \label{fig:teacher_results}
\end{figure}

\subsection{Student Model (Distilled) Performance}
The distilled 4.3M parameter Student model demonstrated exceptional performance retention. While there is slightly more overlap in the predictions compared to the teacher, the model correctly identifies most healthy patients (Figure \ref{fig:student_results}).

% FIGURE 6: STUDENT RESULTS
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{student_results.png}
    \caption{Student Model Results. The distilled model shows slightly more overlap in the center of the heatmap, indicating difficulty differentiating the specific signatures of the two cardiomyopathies, but maintains high accuracy for Non-Failing (NF) predictions \cite{agarwalla2023presentation}.}
    \label{fig:student_results}
\end{figure}

\subsubsection{Pretraining Metrics and Class-wise Analysis}
Quantitatively, the Student model demonstrates remarkable robustness to class imbalance, a common challenge in biological datasets where disease samples are often rare compared to healthy controls.

As detailed in Figure \ref{fig:student_metrics}, the model achieves an *88.4\% Recall* on the minority class (Dilated Cardiomyopathy - DCM). This is a critical result because it indicates the model rarely misses a true DCM case, a high priority for diagnostic screening tools. However, the Precision for DCM is lower (66.3\%), suggesting that while the model catches almost all DCM cases, it occasionally misclassifies other phenotypes (likely HCM, given the biological overlap) as DCM.

Conversely, for the Non-Failing (NF) healthy control group, the model maintains high *Precision (84.2\%)* and *Recall (85.3\%)*. This balance ensures trustworthiness; the model effectively screens out healthy patients without raising excessive false alarms. The high F1-scores across all three classes (87.6\% for HCM, 75.8\% for DCM, 84.7\% for NF) validate that the distillation process preserved the Teacher's ability to distinguish subtle transcriptomic signatures across diverse biological states.

% FIGURE 7: STUDENT METRICS
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{student_metrics.png}
    \caption{Per-Class Performance Metrics for the 4.3M Student Model. The model demonstrates robustness to class imbalance, achieving high recall on the minority DCM class \cite{agarwalla2023presentation}.}
    \label{fig:student_metrics}
\end{figure}

\subsubsection{Downscaling Analysis}
We compared the Teacher model against varying sizes of the Student model (4.3M, 3M, and 2M parameters). As shown in Figure \ref{fig:downscaling}, the 4.3M parameter model retains \textbf{98\%} of the Teacher's performance (84.53\% vs 86.14\%). Even the tiny 2M parameter model remains viable at 79.1\% accuracy, proving that massive compute is not strictly necessary for this task.

% FIGURE 8: DOWNSCALING RESULTS
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{teacher_student.png}
    \caption{Teacher vs. Student Performance Comparison across model sizes. The 4.3M Student model performs nearly identically to the Teacher, validating the efficiency of the distillation process \cite{agarwalla2023presentation}.}
    \label{fig:downscaling}
\end{figure}

% -------------------------------------------------------------------
% 4. DISCUSSION
% -------------------------------------------------------------------
\section{Discussion}

Our reproduction and extension of Geneformer revealed significant insights into both the model's capabilities and the practical challenges of deploying genomic foundation models.

\subsection{Critique of the Original Geneformer}
\textbf{Strengths:}
\begin{itemize}
    \item \textbf{Data Curation:} The aggregation and rigorous filtering of the 30M-cell corpus is a foundational achievement, providing a robust data foundation for transfer learning \cite{agarwalla2023presentation}.
    \item \textbf{Rank Value Encoding:} This non-parametric approach effectively normalizes technical noise and deprioritizes housekeeping genes, forcing the model to focus on biologically relevant signal (transcription factors) \cite{agarwalla2023presentation}.
    \item \textbf{Masking Strategy:} The 15\% masking objective successfully forces the model to learn network topology, as evidenced by the high accuracy in downstream tasks \cite{agarwalla2023presentation}.
    \item \textbf{In Silico Perturbation:} The framework enables therapeutic discovery without wet-lab experimentation, a powerful tool for drug repurposing \cite{agarwalla2023presentation}.
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}
    \item \textbf{Data Availability:} While the pre-tokenized data is provided, the raw transcriptome data is not easily accessible, limiting validation of the preprocessing steps \cite{agarwalla2023presentation}.
    \item \textbf{Input Representation:} Rank value encoding, while robust, discards precise expression magnitude information which might be crucial for subtle phenotype differentiation \cite{agarwalla2023presentation}.
    \item \textbf{Data Bias:} The Genecorpus-30M is heavily skewed towards specific organs (e.g., fetal tissue), potentially biasing predictions for underrepresented tissues \cite{agarwalla2023presentation}.
    \item \textbf{Compute Requirements:} The original training required massive computational resources (12 V100 GPUs), making it inaccessible for most academic labs to retrain or extend \cite{agarwalla2023presentation}.
\end{itemize}

\subsection{Reproducibility Experience}
Reproducing the original results presented significant engineering hurdles:
\begin{itemize}
    \item \textbf{Data Scale:} Handling 30M rows of pre-tokenized data required specialized memory optimization. The original dataset utilized `int64` integers; we converted this to `int16` format, which drastically reduced memory consumption and prevented RAM overflow during loading \cite{agarwalla2023presentation}.
    \item \textbf{Metadata Complexity:} Filtering for specific cell types (e.g., Cardiomyocytes) required complex parsing of heterogeneous metadata across datasets \cite{agarwalla2023presentation}.
    \item \textbf{Hardware Constraints:} Training the 10M parameter model was infeasible on consumer hardware, necessitating the development of our custom data collator and the adoption of Knowledge Distillation \cite{agarwalla2023presentation}.
\end{itemize}

\subsection{Future Work}
To address these limitations and further democratize genomic AI, future research should focus on:
\begin{enumerate}
    \item \textbf{Distillation Hyperparameter Optimization:} We used standard hyper-parameters for distillation, we believe results can be even better if we do hyper parameter optimization. 
    \item \textbf{Bias Mitigation:} Collecting more diverse datasets to balance organ representation in the pretraining corpus would improve generalization to rare tissues.
    \item \textbf{V2 Dataset:} The authors have mentioned a 104M-row V2 dataset. Applying our distillation techniques to this larger corpus could yield even more powerful lightweight models \cite{agarwalla2023presentation}.
\end{enumerate}


% -------------------------------------------------------------------
% CONCLUSION
% -------------------------------------------------------------------
\section{Conclusion}

In this project, we successfully reproduced the Geneformer pipeline and introduced a \textbf{Geneformer Distilled} model that democratizes genomic AI. Despite significant hardware constraints, we compressed the model to 4.3M, 3M and 2M parameters, achieving accuracy comparable to the original 10M-parameter Teacher while utilizing only \textbf{1/25th of the compute} and \textbf{1/20th of the data} \cite{agarwalla2023presentation}.

Our analysis reveals a functional trade-off: while the Teacher acts as an "Aggressive Disease Detector" with higher recall (88\% for HCM), the Student serves as a "Robust Healthy Screener," maintaining high precision for non-failing hearts (85\%) despite slightly reduced granularity in distinguishing specific cardiomyopathies. Ultimately, this work demonstrates that lightweight models can effectively retain the grammar of gene regulation, significantly lowering the barrier to entry for therapeutic discovery in network biology.

\newpage

% -------------------------------------------------------------------
% REFERENCES
% -------------------------------------------------------------------
\printbibliography

\end{document}